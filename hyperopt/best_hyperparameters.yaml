activation: relu
epochs: 1000
initializer: random_normal
learning_rate: 1.0451802250942976
optimizer: adam
units_1: 4.0
units_2: 20.0
